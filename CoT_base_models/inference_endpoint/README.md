# CoT_base_models: inference_endpoint
An inference endpoint that runs on a development (private) server. Includes multiple LLMs and inference comparisons.

## [endpoint_main](./endpoint_main.py)
Initalises local server using Flask and handles inputs and outputs to server.

## [templates/index.html](./templates/index.html)
Server entry point from browser to interact with and prompt LLMs.